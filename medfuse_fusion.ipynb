{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 多模态医学影像Embedding融合\n",
        "\n",
        "基于MedFuse的LSTM-based fusion方法，融合SAM-Med2D、MedCLIP和RadFM的image embedding。\n",
        "\n",
        "## 功能\n",
        "- 融合三种模型的embedding（SAM-Med2D, MedCLIP, RadFM）\n",
        "- CT和PET分别进行fusion\n",
        "- 分别计算进展和死亡的ROC AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "库导入完成！\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的库\n",
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "\n",
        "print(\"库导入完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. 定义MedFuse LSTM融合模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MedFuseLSTM模型定义完成！\n"
          ]
        }
      ],
      "source": [
        "class MedFuseLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    MedFuse LSTM-based fusion module\n",
        "    可以处理单模态或多模态输入\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dims: List[int],\n",
        "        hidden_dim: int = 128,\n",
        "        num_layers: int = 2,\n",
        "        dropout: float = 0.3,\n",
        "        output_dim: int = 1,\n",
        "    ):\n",
        "        super(MedFuseLSTM, self).__init__()\n",
        "        self.num_modalities = len(input_dims)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # 为每个模态创建投影层\n",
        "        self.projection_layers = nn.ModuleList([\n",
        "            nn.Linear(dim, hidden_dim) for dim in input_dims\n",
        "        ])\n",
        "        \n",
        "        # LSTM层\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        \n",
        "        # 输出层\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, output_dim),\n",
        "        )\n",
        "        \n",
        "    def forward(self, modality_embeddings: List[torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            modality_embeddings: List of tensors, each of shape [batch_size, embedding_dim]\n",
        "                                可以是1个或多个模态\n",
        "        Returns:\n",
        "            output: [batch_size, output_dim]\n",
        "        \"\"\"\n",
        "        batch_size = modality_embeddings[0].shape[0]\n",
        "        \n",
        "        # 投影每个模态的embedding\n",
        "        projected = []\n",
        "        for i, emb in enumerate(modality_embeddings):\n",
        "            if emb is not None:\n",
        "                proj = self.projection_layers[i](emb)  # [batch_size, hidden_dim]\n",
        "                projected.append(proj)\n",
        "        \n",
        "        if not projected:\n",
        "            # 如果没有可用模态，返回零向量\n",
        "            return torch.zeros(batch_size, 1, device=modality_embeddings[0].device)\n",
        "        \n",
        "        # 堆叠为序列 [batch_size, num_modalities, hidden_dim]\n",
        "        sequence = torch.stack(projected, dim=1)\n",
        "        \n",
        "        # LSTM处理\n",
        "        lstm_out, (h_n, c_n) = self.lstm(sequence)\n",
        "        \n",
        "        # 使用最后一个时间步的输出\n",
        "        last_output = lstm_out[:, -1, :]  # [batch_size, hidden_dim]\n",
        "        \n",
        "        # 全连接层\n",
        "        output = self.fc(last_output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "print(\"MedFuseLSTM模型定义完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 定义数据集类\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MultiModalDataset定义完成！\n"
          ]
        }
      ],
      "source": [
        "class MultiModalDataset(Dataset):\n",
        "    \"\"\"多模态数据集\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        patient_data: Dict[str, Dict],\n",
        "        labels: Dict[str, int],\n",
        "        modality_keys: List[str],\n",
        "        embedding_dims: Dict[str, int],\n",
        "    ):\n",
        "        self.patient_data = patient_data\n",
        "        self.labels = labels\n",
        "        self.modality_keys = modality_keys\n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.patient_ids = list(patient_data.keys())\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.patient_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        patient_id = self.patient_ids[idx]\n",
        "        data = self.patient_data[patient_id]\n",
        "        label = self.labels.get(patient_id, 0)\n",
        "        \n",
        "        # 提取各模态的embedding\n",
        "        embeddings = []\n",
        "        for key in self.modality_keys:\n",
        "            if key in data and data[key] is not None:\n",
        "                emb_array = data[key]\n",
        "                # 确保是numpy数组\n",
        "                if not isinstance(emb_array, np.ndarray):\n",
        "                    emb_array = np.array(emb_array)\n",
        "                \n",
        "                # 处理不同维度的数组\n",
        "                if emb_array.ndim == 0:\n",
        "                    # 标量，转换为1维数组\n",
        "                    emb_array = np.array([emb_array])\n",
        "                elif emb_array.ndim > 1:\n",
        "                    # 多维数组，展平\n",
        "                    emb_array = emb_array.flatten()\n",
        "                \n",
        "                # 确保维度正确\n",
        "                expected_dim = self.embedding_dims.get(key, emb_array.shape[0] if emb_array.ndim > 0 else 1)\n",
        "                if emb_array.shape[0] != expected_dim:\n",
        "                    # 如果维度不匹配，调整\n",
        "                    if emb_array.shape[0] > expected_dim:\n",
        "                        emb_array = emb_array[:expected_dim]\n",
        "                    else:\n",
        "                        # 用零填充\n",
        "                        padding = np.zeros(expected_dim - emb_array.shape[0], dtype=emb_array.dtype)\n",
        "                        emb_array = np.concatenate([emb_array, padding])\n",
        "                \n",
        "                emb = torch.tensor(emb_array, dtype=torch.float32)\n",
        "            else:\n",
        "                # 如果缺失，使用零向量\n",
        "                dim = self.embedding_dims.get(key, 1)\n",
        "                emb = torch.zeros(dim, dtype=torch.float32)\n",
        "            embeddings.append(emb)\n",
        "        \n",
        "        return {\n",
        "            'embeddings': embeddings,\n",
        "            'label': torch.tensor(label, dtype=torch.float32),\n",
        "            'patient_id': patient_id,\n",
        "        }\n",
        "\n",
        "print(\"MultiModalDataset定义完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 加载Embedding数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load_embeddings函数定义完成！\n"
          ]
        }
      ],
      "source": [
        "def normalize_patient_name(name: str) -> str:\n",
        "    \"\"\"标准化患者名称，用于匹配\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"\"\n",
        "    name = str(name).strip()\n",
        "    # 去除多余空格\n",
        "    name = \" \".join(name.split())\n",
        "    # 统一转换为小写（可选，根据实际情况调整）\n",
        "    # name = name.lower()\n",
        "    return name\n",
        "\n",
        "def load_embeddings(\n",
        "    embedding_paths: Dict[str, str],\n",
        "    patient_key_col: str = 'patient_name',\n",
        ") -> Tuple[Dict[str, Dict[str, np.ndarray]], Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    加载所有embedding文件\n",
        "    \n",
        "    Args:\n",
        "        embedding_paths: {embedding_name: file_path}\n",
        "        patient_key_col: 患者标识列名\n",
        "    \n",
        "    Returns:\n",
        "        (all_embeddings, original_to_normalized): \n",
        "        - all_embeddings: {normalized_patient_id: {embedding_name: embedding_array}}\n",
        "        - original_to_normalized: {original_name: normalized_name} 映射\n",
        "    \"\"\"\n",
        "    all_embeddings = {}\n",
        "    original_to_normalized = {}  # 原始名称到标准化名称的映射\n",
        "    \n",
        "    for emb_name, emb_path in embedding_paths.items():\n",
        "        logging.info(f\"加载 {emb_name} from {emb_path}\")\n",
        "        \n",
        "        if emb_path.endswith('.npz'):\n",
        "            data = np.load(emb_path, allow_pickle=True)\n",
        "            \n",
        "            # 尝试不同的键名\n",
        "            if 'embeddings' in data:\n",
        "                embeddings = data['embeddings']\n",
        "            elif 'embedding' in data:\n",
        "                embeddings = data['embedding']\n",
        "            else:\n",
        "                # 取第一个数组\n",
        "                keys = list(data.keys())\n",
        "                embeddings = data[keys[0]]\n",
        "            \n",
        "            # 获取患者名称\n",
        "            if 'patient_names' in data:\n",
        "                patient_names = data['patient_names']\n",
        "            elif 'patient_name' in data:\n",
        "                patient_names = data['patient_name']\n",
        "            elif patient_key_col in data:\n",
        "                patient_names = data[patient_key_col]\n",
        "            else:\n",
        "                # 尝试从CSV读取\n",
        "                csv_path = emb_path.replace('.npz', '.csv')\n",
        "                if os.path.exists(csv_path):\n",
        "                    df = pd.read_csv(csv_path, nrows=1000)  # 只读前1000行来检查\n",
        "                    if patient_key_col in df.columns:\n",
        "                        # 重新读取完整文件\n",
        "                        df = pd.read_csv(csv_path)\n",
        "                        embeddings = df.drop(columns=[patient_key_col, 'center'] if 'center' in df.columns else [patient_key_col]).values\n",
        "                        patient_names = df[patient_key_col].values\n",
        "                    else:\n",
        "                        logging.warning(f\"无法找到患者名称列，跳过 {emb_name}\")\n",
        "                        continue\n",
        "                else:\n",
        "                    logging.warning(f\"无法找到患者名称，跳过 {emb_name}\")\n",
        "                    continue\n",
        "            \n",
        "            # 处理patient_names并标准化\n",
        "            if isinstance(patient_names, np.ndarray):\n",
        "                if patient_names.dtype == object:\n",
        "                    patient_names = [str(p) for p in patient_names]\n",
        "                else:\n",
        "                    patient_names = patient_names.astype(str)\n",
        "            else:\n",
        "                patient_names = [str(p) for p in patient_names]\n",
        "            \n",
        "            # 存储每个患者的embedding（使用标准化名称）\n",
        "            # 对于RadFM，同一个患者可能有多个embedding（多个token），需要聚合\n",
        "            patient_embeddings_dict = {}  # {patient_id: [emb1, emb2, ...]}\n",
        "            \n",
        "            for i, original_name in enumerate(patient_names):\n",
        "                normalized_name = normalize_patient_name(original_name)\n",
        "                if not normalized_name:\n",
        "                    continue\n",
        "                \n",
        "                # 保存映射\n",
        "                original_to_normalized[original_name] = normalized_name\n",
        "                \n",
        "                # 如果是2D数组，取第i行\n",
        "                if embeddings.ndim == 2:\n",
        "                    emb = embeddings[i]\n",
        "                else:\n",
        "                    emb = embeddings[i] if i < len(embeddings) else None\n",
        "                \n",
        "                if emb is not None:\n",
        "                    if normalized_name not in patient_embeddings_dict:\n",
        "                        patient_embeddings_dict[normalized_name] = []\n",
        "                    patient_embeddings_dict[normalized_name].append(emb)\n",
        "            \n",
        "            # 聚合同一患者的多个embedding（对于RadFM等）\n",
        "            for normalized_name, emb_list in patient_embeddings_dict.items():\n",
        "                if normalized_name not in all_embeddings:\n",
        "                    all_embeddings[normalized_name] = {}\n",
        "                \n",
        "                if len(emb_list) == 1:\n",
        "                    # 只有一个embedding，直接使用\n",
        "                    all_embeddings[normalized_name][emb_name] = emb_list[0]\n",
        "                else:\n",
        "                    # 多个embedding，进行平均池化\n",
        "                    emb_array = np.array(emb_list)\n",
        "                    # 如果是3D数组 [num_tokens, embedding_dim]，平均池化\n",
        "                    if emb_array.ndim == 2:\n",
        "                        # 对token维度求平均\n",
        "                        aggregated_emb = np.mean(emb_array, axis=0)\n",
        "                    else:\n",
        "                        # 展平后求平均\n",
        "                        aggregated_emb = np.mean(emb_array.flatten())\n",
        "                    all_embeddings[normalized_name][emb_name] = aggregated_emb\n",
        "        \n",
        "        elif emb_path.endswith('.xlsx') or emb_path.endswith('.xls'):\n",
        "            # 处理Excel文件（SAM-Med2D）\n",
        "            try:\n",
        "                df = pd.read_excel(emb_path, engine='openpyxl')\n",
        "                all_columns = df.columns.tolist()\n",
        "                logging.info(f\"  Excel文件列名: {all_columns[:10]}... (共{len(all_columns)}列)\")\n",
        "                \n",
        "                # 尝试找到患者名称列\n",
        "                patient_col = None\n",
        "                for col in all_columns:\n",
        "                    col_lower = str(col).lower()\n",
        "                    if 'patient' in col_lower or 'name' in col_lower or '患者' in str(col) or '姓名' in str(col):\n",
        "                        patient_col = col\n",
        "                        break\n",
        "                \n",
        "                # 如果找不到，使用第一列\n",
        "                if patient_col is None:\n",
        "                    patient_col = all_columns[0]\n",
        "                    logging.info(f\"  未找到患者名称列，使用第一列: {patient_col}\")\n",
        "                else:\n",
        "                    logging.info(f\"  使用患者名称列: {patient_col}\")\n",
        "                \n",
        "                # 提取embedding列（排除患者名称列和center列）\n",
        "                exclude_cols = [patient_col, 'center', '中心', 'file_key', 'Center']\n",
        "                emb_cols = [col for col in df.columns if col not in exclude_cols]\n",
        "                \n",
        "                # 进一步过滤：只保留数值列\n",
        "                numeric_cols = []\n",
        "                for col in emb_cols:\n",
        "                    col_str = str(col).lower()\n",
        "                    if col_str.startswith('feature_') or col_str.startswith('embedding_') or col_str.replace('_', '').replace('.', '').isdigit():\n",
        "                        numeric_cols.append(col)\n",
        "                \n",
        "                if not numeric_cols:\n",
        "                    # 如果没找到特征列，尝试所有非排除列\n",
        "                    numeric_cols = emb_cols\n",
        "                    logging.info(f\"  使用所有非排除列作为embedding列（共{len(numeric_cols)}列）\")\n",
        "                \n",
        "                if not numeric_cols:\n",
        "                    logging.warning(f\"  未找到embedding列\")\n",
        "                    continue\n",
        "                \n",
        "                # 转换数值列\n",
        "                emb_data = df[numeric_cols].copy()\n",
        "                for col in emb_data.columns:\n",
        "                    emb_data[col] = pd.to_numeric(emb_data[col], errors='coerce')\n",
        "                emb_data = emb_data.fillna(0)\n",
        "                \n",
        "                embeddings_array = emb_data.values.astype(np.float32)\n",
        "                patient_names = df[patient_col].astype(str).values\n",
        "                \n",
        "                # 对于RadFM，同一个患者可能有多个embedding，需要聚合\n",
        "                patient_embeddings_dict = {}  # {patient_id: [emb1, emb2, ...]}\n",
        "                \n",
        "                for i, original_name in enumerate(patient_names):\n",
        "                    normalized_name = normalize_patient_name(original_name)\n",
        "                    if not normalized_name:\n",
        "                        continue\n",
        "                    \n",
        "                    # 保存映射\n",
        "                    original_to_normalized[original_name] = normalized_name\n",
        "                    \n",
        "                    emb = embeddings_array[i]\n",
        "                    if normalized_name not in patient_embeddings_dict:\n",
        "                        patient_embeddings_dict[normalized_name] = []\n",
        "                    patient_embeddings_dict[normalized_name].append(emb)\n",
        "                \n",
        "                # 聚合同一患者的多个embedding\n",
        "                for normalized_name, emb_list in patient_embeddings_dict.items():\n",
        "                    if normalized_name not in all_embeddings:\n",
        "                        all_embeddings[normalized_name] = {}\n",
        "                    \n",
        "                    if len(emb_list) == 1:\n",
        "                        # 只有一个embedding，直接使用\n",
        "                        emb = emb_list[0]\n",
        "                        if not isinstance(emb, np.ndarray):\n",
        "                            emb = np.array(emb, dtype=np.float32)\n",
        "                        all_embeddings[normalized_name][emb_name] = emb\n",
        "                    else:\n",
        "                        # 多个embedding，进行平均池化\n",
        "                        try:\n",
        "                            emb_arrays = []\n",
        "                            for emb in emb_list:\n",
        "                                if not isinstance(emb, np.ndarray):\n",
        "                                    emb = np.array(emb, dtype=np.float32)\n",
        "                                emb_arrays.append(emb)\n",
        "                            \n",
        "                            emb_array = np.array(emb_arrays)\n",
        "                            if emb_array.dtype == object:\n",
        "                                emb_array = np.array([np.array(e, dtype=np.float32) for e in emb_arrays])\n",
        "                            \n",
        "                            if emb_array.ndim == 2:\n",
        "                                aggregated_emb = np.mean(emb_array, axis=0).astype(np.float32)\n",
        "                            elif emb_array.ndim > 2:\n",
        "                                aggregated_emb = np.mean(emb_array.flatten()).astype(np.float32)\n",
        "                            else:\n",
        "                                aggregated_emb = np.mean(emb_array).astype(np.float32)\n",
        "                            \n",
        "                            all_embeddings[normalized_name][emb_name] = aggregated_emb\n",
        "                        except Exception as e:\n",
        "                            logging.warning(f\"  聚合患者 {normalized_name} 的embedding失败: {e}\")\n",
        "                            if emb_list:\n",
        "                                emb = emb_list[0]\n",
        "                                if not isinstance(emb, np.ndarray):\n",
        "                                    emb = np.array(emb, dtype=np.float32)\n",
        "                                all_embeddings[normalized_name][emb_name] = emb\n",
        "                \n",
        "                logging.info(f\"  {emb_name}: 加载了 {len(set(original_to_normalized.values()))} 个唯一患者\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                logging.error(f\"读取Excel文件 {emb_path} 失败: {e}\")\n",
        "                continue\n",
        "        \n",
        "        elif emb_path.endswith('.csv'):\n",
        "            # 先读取第一行来检查列名\n",
        "            try:\n",
        "                sample_df = pd.read_csv(emb_path, nrows=5)\n",
        "                all_columns = sample_df.columns.tolist()\n",
        "                logging.info(f\"  CSV文件列名: {all_columns[:10]}... (共{len(all_columns)}列)\")\n",
        "                \n",
        "                # 尝试找到患者名称列\n",
        "                patient_col = None\n",
        "                for col in all_columns:\n",
        "                    col_lower = str(col).lower()\n",
        "                    if 'patient' in col_lower or 'name' in col_lower or '患者' in str(col) or '姓名' in str(col):\n",
        "                        patient_col = col\n",
        "                        break\n",
        "                \n",
        "                # 如果找不到，使用第一列\n",
        "                if patient_col is None:\n",
        "                    patient_col = all_columns[0]\n",
        "                    logging.info(f\"  未找到患者名称列，使用第一列: {patient_col}\")\n",
        "                else:\n",
        "                    logging.info(f\"  使用患者名称列: {patient_col}\")\n",
        "                \n",
        "            except Exception as e:\n",
        "                logging.error(f\"  读取CSV文件头失败: {e}\")\n",
        "                continue\n",
        "            \n",
        "            # 对于大文件，分块读取\n",
        "            chunk_size = 1000\n",
        "            patient_names = []\n",
        "            embeddings_list = []\n",
        "            \n",
        "            try:\n",
        "                for chunk in pd.read_csv(emb_path, chunksize=chunk_size):\n",
        "                    if patient_col not in chunk.columns:\n",
        "                        logging.warning(f\"CSV文件 {emb_path} 中未找到 {patient_col} 列\")\n",
        "                        break\n",
        "                    \n",
        "                    # 提取embedding列（排除患者名称列、center列和其他非数值列）\n",
        "                    exclude_cols = [patient_col, 'center', '中心', 'file_key', 'Center']\n",
        "                    emb_cols = [col for col in chunk.columns if col not in exclude_cols]\n",
        "                    \n",
        "                    # 进一步过滤：只保留数值列（feature_开头或embedding_开头或纯数字列名）\n",
        "                    numeric_cols = []\n",
        "                    for col in emb_cols:\n",
        "                        col_str = str(col).lower()\n",
        "                        if col_str.startswith('feature_') or col_str.startswith('embedding_') or col_str.replace('_', '').replace('.', '').isdigit():\n",
        "                            numeric_cols.append(col)\n",
        "                    \n",
        "                    if not numeric_cols:\n",
        "                        # 如果没找到特征列，尝试所有非排除列\n",
        "                        numeric_cols = emb_cols\n",
        "                        logging.info(f\"  使用所有非排除列作为embedding列（共{len(numeric_cols)}列）\")\n",
        "                    \n",
        "                    if not numeric_cols:\n",
        "                        logging.warning(f\"  未找到embedding列\")\n",
        "                        break\n",
        "                    \n",
        "                    # 确保只选择数值列并转换类型\n",
        "                    try:\n",
        "                        # 先尝试选择数值类型\n",
        "                        emb_data = chunk[numeric_cols].select_dtypes(include=[np.number])\n",
        "                        \n",
        "                        # 如果为空或列数不对，手动转换\n",
        "                        if emb_data.empty or len(emb_data.columns) != len(numeric_cols):\n",
        "                            # 逐列转换，将非数值转换为NaN\n",
        "                            emb_data = chunk[numeric_cols].copy()\n",
        "                            for col in emb_data.columns:\n",
        "                                emb_data[col] = pd.to_numeric(emb_data[col], errors='coerce')\n",
        "                            # 填充NaN为0\n",
        "                            emb_data = emb_data.fillna(0)\n",
        "                        \n",
        "                        # 转换为numpy数组并确保是float32\n",
        "                        emb_values = emb_data.values.astype(np.float32)\n",
        "                        embeddings_list.append(emb_values)\n",
        "                        patient_names.extend(chunk[patient_col].astype(str).values)\n",
        "                        \n",
        "                    except Exception as e:\n",
        "                        logging.warning(f\"  处理embedding列时出错: {e}，尝试备用方法\")\n",
        "                        # 备用方法：逐列处理\n",
        "                        try:\n",
        "                            emb_list = []\n",
        "                            for col in numeric_cols:\n",
        "                                col_data = pd.to_numeric(chunk[col], errors='coerce').fillna(0).values\n",
        "                                emb_list.append(col_data)\n",
        "                            \n",
        "                            if emb_list:\n",
        "                                emb_array = np.column_stack(emb_list).astype(np.float32)\n",
        "                                embeddings_list.append(emb_array)\n",
        "                                patient_names.extend(chunk[patient_col].astype(str).values)\n",
        "                            else:\n",
        "                                logging.error(f\"  无法提取任何数值列\")\n",
        "                                break\n",
        "                        except Exception as e2:\n",
        "                            logging.error(f\"  备用方法也失败: {e2}\")\n",
        "                            break\n",
        "                \n",
        "                if embeddings_list:\n",
        "                    embeddings_array = np.vstack(embeddings_list)\n",
        "                    \n",
        "                    # 对于RadFM，同一个患者可能有多个embedding，需要聚合\n",
        "                    patient_embeddings_dict = {}  # {patient_id: [emb1, emb2, ...]}\n",
        "                    \n",
        "                    for i, original_name in enumerate(patient_names):\n",
        "                        normalized_name = normalize_patient_name(original_name)\n",
        "                        if not normalized_name:\n",
        "                            continue\n",
        "                        \n",
        "                        # 保存映射\n",
        "                        original_to_normalized[original_name] = normalized_name\n",
        "                        \n",
        "                        emb = embeddings_array[i]\n",
        "                        if normalized_name not in patient_embeddings_dict:\n",
        "                            patient_embeddings_dict[normalized_name] = []\n",
        "                        patient_embeddings_dict[normalized_name].append(emb)\n",
        "                    \n",
        "                    # 聚合同一患者的多个embedding\n",
        "                    for normalized_name, emb_list in patient_embeddings_dict.items():\n",
        "                        if normalized_name not in all_embeddings:\n",
        "                            all_embeddings[normalized_name] = {}\n",
        "                        \n",
        "                        if len(emb_list) == 1:\n",
        "                            # 只有一个embedding，直接使用\n",
        "                            emb = emb_list[0]\n",
        "                            # 确保是numpy数组\n",
        "                            if not isinstance(emb, np.ndarray):\n",
        "                                emb = np.array(emb, dtype=np.float32)\n",
        "                            all_embeddings[normalized_name][emb_name] = emb\n",
        "                        else:\n",
        "                            # 多个embedding，进行平均池化\n",
        "                            try:\n",
        "                                # 确保所有embedding都是numpy数组\n",
        "                                emb_arrays = []\n",
        "                                for emb in emb_list:\n",
        "                                    if not isinstance(emb, np.ndarray):\n",
        "                                        emb = np.array(emb, dtype=np.float32)\n",
        "                                    emb_arrays.append(emb)\n",
        "                                \n",
        "                                emb_array = np.array(emb_arrays)\n",
        "                                # 确保是数值类型\n",
        "                                if emb_array.dtype == object:\n",
        "                                    # 如果包含字符串，尝试转换\n",
        "                                    emb_array = np.array([np.array(e, dtype=np.float32) for e in emb_arrays])\n",
        "                                \n",
        "                                if emb_array.ndim == 2:\n",
        "                                    # 对token维度求平均\n",
        "                                    aggregated_emb = np.mean(emb_array, axis=0).astype(np.float32)\n",
        "                                elif emb_array.ndim > 2:\n",
        "                                    # 多维数组，展平后求平均\n",
        "                                    aggregated_emb = np.mean(emb_array.flatten()).astype(np.float32)\n",
        "                                else:\n",
        "                                    # 一维数组，直接求平均\n",
        "                                    aggregated_emb = np.mean(emb_array).astype(np.float32)\n",
        "                                \n",
        "                                all_embeddings[normalized_name][emb_name] = aggregated_emb\n",
        "                            except Exception as e:\n",
        "                                logging.warning(f\"  聚合患者 {normalized_name} 的embedding失败: {e}\")\n",
        "                                # 如果聚合失败，使用第一个embedding\n",
        "                                if emb_list:\n",
        "                                    emb = emb_list[0]\n",
        "                                    if not isinstance(emb, np.ndarray):\n",
        "                                        emb = np.array(emb, dtype=np.float32)\n",
        "                                    all_embeddings[normalized_name][emb_name] = emb\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"读取CSV文件 {emb_path} 失败: {e}\")\n",
        "                continue\n",
        "        \n",
        "        logging.info(f\"  {emb_name}: 加载了 {len(set(original_to_normalized.values()))} 个唯一患者\")\n",
        "    \n",
        "    return all_embeddings, original_to_normalized\n",
        "\n",
        "print(\"load_embeddings函数定义完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 加载标签数据（进展和死亡）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load_labels_from_excel函数定义完成！\n"
          ]
        }
      ],
      "source": [
        "def load_labels_from_excel(excel_path: str) -> Tuple[Dict[str, int], Dict[str, int], Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    从Excel文件加载标签（进展/死亡）\n",
        "    返回两个字典：进展标签和死亡标签，以及原始名称到标准化名称的映射\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_excel(excel_path, engine='openpyxl')\n",
        "        logging.info(f\"从 {excel_path} 加载标签，列名: {df.columns.tolist()}\")\n",
        "        \n",
        "        # 尝试找到患者名称列和标签列\n",
        "        patient_col = None\n",
        "        progress_col = None\n",
        "        death_col = None\n",
        "        \n",
        "        for col in df.columns:\n",
        "            col_lower = str(col).lower()\n",
        "            col_str = str(col)\n",
        "            if '患者' in col_str or 'patient' in col_str or '姓名' in col_str or 'name' in col_str:\n",
        "                patient_col = col\n",
        "            if '进展' in col_str:\n",
        "                progress_col = col\n",
        "            if '死亡' in col_str:\n",
        "                death_col = col\n",
        "        \n",
        "        if patient_col is None:\n",
        "            logging.warning(f\"未找到患者列，使用第一列作为患者名\")\n",
        "            patient_col = df.columns[0]\n",
        "        \n",
        "        progress_labels = {}\n",
        "        death_labels = {}\n",
        "        original_to_normalized = {}\n",
        "        \n",
        "        for _, row in df.iterrows():\n",
        "            original_patient_id = str(row[patient_col])\n",
        "            patient_id = normalize_patient_name(original_patient_id)\n",
        "            \n",
        "            if not patient_id:\n",
        "                continue\n",
        "            \n",
        "            # 保存映射\n",
        "            original_to_normalized[original_patient_id] = patient_id\n",
        "            \n",
        "            # 处理进展标签\n",
        "            if progress_col and progress_col in row:\n",
        "                progress_val = row[progress_col]\n",
        "                if not pd.isna(progress_val):\n",
        "                    if isinstance(progress_val, (int, float)):\n",
        "                        progress_labels[patient_id] = int(progress_val)\n",
        "                    elif isinstance(progress_val, str):\n",
        "                        if '进展' in progress_val or '1' in progress_val or 'yes' in progress_val.lower():\n",
        "                            progress_labels[patient_id] = 1\n",
        "                        else:\n",
        "                            progress_labels[patient_id] = 0\n",
        "                    else:\n",
        "                        progress_labels[patient_id] = int(progress_val)\n",
        "            \n",
        "            # 处理死亡标签\n",
        "            if death_col and death_col in row:\n",
        "                death_val = row[death_col]\n",
        "                if not pd.isna(death_val):\n",
        "                    if isinstance(death_val, (int, float)):\n",
        "                        death_labels[patient_id] = int(death_val)\n",
        "                    elif isinstance(death_val, str):\n",
        "                        if '死亡' in death_val or '1' in death_val or 'yes' in death_val.lower():\n",
        "                            death_labels[patient_id] = 1\n",
        "                        else:\n",
        "                            death_labels[patient_id] = 0\n",
        "                    else:\n",
        "                        death_labels[patient_id] = int(death_val)\n",
        "        \n",
        "        logging.info(f\"进展标签: {len(progress_labels)} 个，正样本: {sum(progress_labels.values())}\")\n",
        "        logging.info(f\"死亡标签: {len(death_labels)} 个，正样本: {sum(death_labels.values())}\")\n",
        "        return progress_labels, death_labels, original_to_normalized\n",
        "    \n",
        "    except Exception as e:\n",
        "        logging.error(f\"加载标签失败: {e}\")\n",
        "        return {}, {}, {}\n",
        "\n",
        "print(\"load_labels_from_excel函数定义完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 配置参数和加载数据\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "配置完成！\n",
            "  模态: CT\n",
            "  标签类型: progress\n",
            "  设备: cuda\n"
          ]
        }
      ],
      "source": [
        "# ========== 配置参数 ==========\n",
        "MODALITY = \"ct\"  # \"ct\" 或 \"pet\"\n",
        "LABEL_TYPE = \"progress\"  # \"progress\" 或 \"death\"\n",
        "EMBEDDING_DIR = \".\"\n",
        "LABEL_FILE = \"名单.xlsx\"\n",
        "OUTPUT_DIR = \"fusion_results\"\n",
        "\n",
        "# 训练参数\n",
        "HIDDEN_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.3\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "LR = 0.001\n",
        "TEST_SIZE = 0.2\n",
        "VAL_SIZE = 0.2\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"配置完成！\")\n",
        "print(f\"  模态: {MODALITY.upper()}\")\n",
        "print(f\"  标签类型: {LABEL_TYPE}\")\n",
        "print(f\"  设备: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "定义的embedding文件路径:\n",
            "  sam_med2d: .\\embeddings_output_CT.xlsx\n",
            "  medclip: .\\medclip_embeddings\\medclip_embeddings_ct_vit.csv\n",
            "  radfm: .\\ct_embeddings_tokens.csv\n",
            "找到 3 个embedding文件:\n",
            "  sam_med2d: .\\embeddings_output_CT.xlsx\n",
            "  medclip: .\\medclip_embeddings\\medclip_embeddings_ct_vit.csv\n",
            "  radfm: .\\ct_embeddings_tokens.csv\n"
          ]
        }
      ],
      "source": [
        "# ========== 定义embedding文件路径 ==========\n",
        "# SAM-Med2D使用xlsx，其他使用CSV\n",
        "if MODALITY.lower() == \"ct\":\n",
        "    embedding_paths = {\n",
        "        \"sam_med2d\": os.path.join(EMBEDDING_DIR, \"embeddings_output_CT.xlsx\"),  # 注意是复数形式embeddings\n",
        "        \"medclip\": os.path.join(EMBEDDING_DIR, \"medclip_embeddings\", \"medclip_embeddings_ct_vit.csv\"),\n",
        "        \"radfm\": os.path.join(EMBEDDING_DIR, \"ct_embeddings_tokens.csv\"),\n",
        "    }\n",
        "else:  # PET\n",
        "    embedding_paths = {\n",
        "        \"sam_med2d\": os.path.join(EMBEDDING_DIR, \"embeddings_output.xlsx\"),  # 注意是复数形式embeddings\n",
        "        \"medclip\": os.path.join(EMBEDDING_DIR, \"medclip_embeddings\", \"medclip_embeddings_vit_dropcol.csv\"),\n",
        "        \"radfm\": os.path.join(EMBEDDING_DIR, \"pet_embedding_tokens_with_labels.csv\"),\n",
        "    }\n",
        "\n",
        "print(\"定义的embedding文件路径:\")\n",
        "for emb_name, path in embedding_paths.items():\n",
        "    print(f\"  {emb_name}: {path}\")\n",
        "\n",
        "# 检查文件是否存在\n",
        "existing_paths = {}\n",
        "for emb_name, path in embedding_paths.items():\n",
        "    if os.path.exists(path):\n",
        "        existing_paths[emb_name] = path\n",
        "    else:\n",
        "        print(f\"警告: 未找到 {emb_name} 文件: {path}\")\n",
        "\n",
        "if not existing_paths:\n",
        "    print(\"错误: 未找到任何embedding文件\")\n",
        "    print(\"尝试查找的文件:\")\n",
        "    for emb_name, path in embedding_paths.items():\n",
        "        print(f\"  {emb_name}: {path}\")\n",
        "else:\n",
        "    print(f\"找到 {len(existing_paths)} 个embedding文件:\")\n",
        "    for emb_name, path in existing_paths.items():\n",
        "        print(f\"  {emb_name}: {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 16:09:52,351 - INFO - 加载 sam_med2d from .\\embeddings_output_CT.xlsx\n",
            "2025-11-29 16:10:34,371 - INFO -   Excel文件列名: ['医院', '患者名称', 'file_name', 'embedding_dim', 'embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5']... (共16383列)\n",
            "2025-11-29 16:10:34,373 - INFO -   使用患者名称列: 患者名称\n",
            "2025-11-29 16:10:38,099 - INFO -   sam_med2d: 加载了 163 个唯一患者\n",
            "2025-11-29 16:10:38,100 - INFO -   sam_med2d: 加载了 163 个唯一患者\n",
            "2025-11-29 16:10:38,101 - INFO - 加载 medclip from .\\medclip_embeddings\\medclip_embeddings_ct_vit.csv\n",
            "2025-11-29 16:10:38,111 - INFO -   CSV文件列名: ['patient_name', 'center', 'embedding_000', 'embedding_001', 'embedding_002', 'embedding_003', 'embedding_004', 'embedding_005', 'embedding_006', 'embedding_007']... (共514列)\n",
            "2025-11-29 16:10:38,112 - INFO -   使用患者名称列: patient_name\n",
            "2025-11-29 16:10:40,827 - INFO -   medclip: 加载了 163 个唯一患者\n",
            "2025-11-29 16:10:40,828 - INFO - 加载 radfm from .\\ct_embeddings_tokens.csv\n",
            "2025-11-29 16:10:40,886 - INFO -   CSV文件列名: ['中心', '患者名', 'file_key', 'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6']... (共5123列)\n",
            "2025-11-29 16:10:40,888 - INFO -   使用患者名称列: 患者名\n",
            "2025-11-29 16:10:45,122 - INFO -   radfm: 加载了 163 个唯一患者\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "总共加载了 163 个患者的embedding数据\n",
            "  sam_med2d: 163 个患者\n",
            "    患者名称示例: ['20211227zhoulinmei', '20160926fuzhongyou', '20170215ruanjieliu']\n",
            "  medclip: 109 个患者\n",
            "    患者名称示例: ['20211227zhoulinmei', '20160926fuzhongyou', '20170215ruanjieliu']\n",
            "  radfm: 158 个患者\n",
            "    患者名称示例: ['20160926fuzhongyou', '20170215ruanjieliu']\n",
            "    embedding形状: (5120,), dtype: float32\n"
          ]
        }
      ],
      "source": [
        "# ========== 加载embeddings ==========\n",
        "all_embeddings, emb_name_mapping = load_embeddings(existing_paths)\n",
        "print(f\"\\n总共加载了 {len(all_embeddings)} 个患者的embedding数据\")\n",
        "\n",
        "# 统计每个模态的患者数量\n",
        "for emb_name in existing_paths.keys():\n",
        "    count = sum(1 for pid, emb_dict in all_embeddings.items() if emb_name in emb_dict)\n",
        "    print(f\"  {emb_name}: {count} 个患者\")\n",
        "    \n",
        "    # 显示一些患者名称示例\n",
        "    sample_patients = [pid for pid, emb_dict in list(all_embeddings.items())[:3] if emb_name in emb_dict]\n",
        "    if sample_patients:\n",
        "        print(f\"    患者名称示例: {sample_patients}\")\n",
        "    \n",
        "    # 对于RadFM，显示一些示例信息\n",
        "    if 'radfm' in emb_name.lower() and sample_patients:\n",
        "        sample_pid = sample_patients[0]\n",
        "        sample_emb = all_embeddings[sample_pid][emb_name]\n",
        "        if hasattr(sample_emb, 'shape'):\n",
        "            print(f\"    embedding形状: {sample_emb.shape}, dtype: {sample_emb.dtype}\")\n",
        "        else:\n",
        "            print(f\"    embedding类型: {type(sample_emb)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 16:10:45,173 - INFO - 从 名单.xlsx 加载标签，列名: ['医院', '姓名', '进展', '死亡']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 16:10:45,182 - INFO - 进展标签: 161 个，正样本: 87\n",
            "2025-11-29 16:10:45,183 - INFO - 死亡标签: 161 个，正样本: 25\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "使用标签: 进展\n",
            "标签数量: 161, 正样本: 87\n"
          ]
        }
      ],
      "source": [
        "# ========== 加载标签 ==========\n",
        "progress_labels, death_labels, label_name_mapping = load_labels_from_excel(LABEL_FILE)\n",
        "\n",
        "# 选择要使用的标签\n",
        "if LABEL_TYPE == \"progress\":\n",
        "    labels = progress_labels\n",
        "    label_name = \"进展\"\n",
        "else:\n",
        "    labels = death_labels\n",
        "    label_name = \"死亡\"\n",
        "\n",
        "print(f\"\\n使用标签: {label_name}\")\n",
        "print(f\"标签数量: {len(labels)}, 正样本: {sum(labels.values())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "数据匹配情况:\n",
            "  Embedding中的患者数: 163\n",
            "  标签中的患者数: 161\n",
            "  总患者数（并集）: 195\n",
            "  准备使用的患者数: 195\n",
            "  同时有embedding和标签: 129\n",
            "  只有embedding（无标签，使用默认标签0）: 34\n",
            "  只有标签（无embedding，用零向量填充）: 32\n",
            "\n",
            "  只有标签的患者示例（前5个）: ['z507468', 'z560929', 'z824491', 'z632091', 'z681396']\n",
            "  只有embedding的患者示例（前5个）: ['20180625yangshiyi', '20180828hexiaoxian', '20180808limengjie', '20170215ruanjieliu', '20191125zhangyehong']\n",
            "\n",
            "准备训练数据: 195 个患者 (进展)\n",
            "  有标签的患者: 161\n",
            "  有embedding的患者: 163\n"
          ]
        }
      ],
      "source": [
        "# ========== 准备数据 ==========\n",
        "# 使用所有患者，即使不匹配也保留\n",
        "patient_data = {}\n",
        "patient_labels = {}\n",
        "\n",
        "# 获取所有患者ID（embedding和标签的并集）\n",
        "all_patient_ids = set(all_embeddings.keys()) | set(labels.keys())\n",
        "\n",
        "# 优先使用有embedding的患者，即使没有标签也保留（用默认标签0）\n",
        "# 对于只有标签的患者，用零向量填充embedding\n",
        "for patient_id in all_patient_ids:\n",
        "    # 只要有embedding就保留\n",
        "    if patient_id in all_embeddings:\n",
        "        patient_data[patient_id] = all_embeddings[patient_id]\n",
        "        # 如果有标签就用标签，没有就用0\n",
        "        patient_labels[patient_id] = labels.get(patient_id, 0)\n",
        "    elif patient_id in labels:\n",
        "        # 只有标签没有embedding，创建空的embedding字典（后续会用零向量填充）\n",
        "        patient_data[patient_id] = {}\n",
        "        patient_labels[patient_id] = labels[patient_id]\n",
        "\n",
        "# 诊断信息\n",
        "matched_with_both = sum(1 for pid in patient_data.keys() if pid in all_embeddings and pid in labels)\n",
        "only_embeddings = sum(1 for pid in patient_data.keys() if pid in all_embeddings and pid not in labels)\n",
        "only_labels = sum(1 for pid in patient_data.keys() if pid not in all_embeddings and pid in labels)\n",
        "\n",
        "print(f\"\\n数据匹配情况:\")\n",
        "print(f\"  Embedding中的患者数: {len(all_embeddings)}\")\n",
        "print(f\"  标签中的患者数: {len(labels)}\")\n",
        "print(f\"  总患者数（并集）: {len(all_patient_ids)}\")\n",
        "print(f\"  准备使用的患者数: {len(patient_data)}\")\n",
        "print(f\"  同时有embedding和标签: {matched_with_both}\")\n",
        "print(f\"  只有embedding（无标签，使用默认标签0）: {only_embeddings}\")\n",
        "print(f\"  只有标签（无embedding，用零向量填充）: {only_labels}\")\n",
        "\n",
        "# 显示一些不匹配的患者示例\n",
        "if only_labels > 0:\n",
        "    only_label_patients = [pid for pid in patient_data.keys() if pid not in all_embeddings and pid in labels][:5]\n",
        "    print(f\"\\n  只有标签的患者示例（前5个）: {only_label_patients}\")\n",
        "\n",
        "if only_embeddings > 0:\n",
        "    only_emb_patients = [pid for pid in patient_data.keys() if pid in all_embeddings and pid not in labels][:5]\n",
        "    print(f\"  只有embedding的患者示例（前5个）: {only_emb_patients}\")\n",
        "\n",
        "if not patient_data:\n",
        "    print(\"错误: 没有找到匹配的患者数据\")\n",
        "else:\n",
        "    print(f\"\\n准备训练数据: {len(patient_data)} 个患者 ({label_name})\")\n",
        "    print(f\"  有标签的患者: {matched_with_both + only_labels}\")\n",
        "    print(f\"  有embedding的患者: {matched_with_both + only_embeddings}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  sam_med2d: 16380 维\n",
            "  medclip: 512 维\n",
            "  radfm: 5120 维\n",
            "\n",
            "Embedding维度: {'sam_med2d': 16380, 'medclip': 512, 'radfm': 5120}\n"
          ]
        }
      ],
      "source": [
        "# ========== 获取embedding维度 ==========\n",
        "modality_keys = list(existing_paths.keys())\n",
        "embedding_dims = {}\n",
        "input_dims = []\n",
        "\n",
        "# 从所有患者中找到每个模态的维度\n",
        "for key in modality_keys:\n",
        "    dim = None\n",
        "    # 优先从有embedding的患者中找\n",
        "    for patient_id, emb_dict in patient_data.items():\n",
        "        if key in emb_dict and emb_dict[key] is not None:\n",
        "            emb_array = emb_dict[key]\n",
        "            if not isinstance(emb_array, np.ndarray):\n",
        "                emb_array = np.array(emb_array)\n",
        "            \n",
        "            # 处理维度\n",
        "            if emb_array.ndim == 0:\n",
        "                dim = 1  # 标量\n",
        "            elif emb_array.ndim == 1:\n",
        "                dim = emb_array.shape[0]  # 一维数组\n",
        "            else:\n",
        "                dim = emb_array.size  # 多维数组，展平后的维度\n",
        "            \n",
        "            if dim is not None and dim > 0:\n",
        "                break\n",
        "    \n",
        "    # 如果还是找不到，尝试从all_embeddings中找\n",
        "    if dim is None:\n",
        "        for patient_id, emb_dict in all_embeddings.items():\n",
        "            if key in emb_dict and emb_dict[key] is not None:\n",
        "                emb_array = emb_dict[key]\n",
        "                if not isinstance(emb_array, np.ndarray):\n",
        "                    emb_array = np.array(emb_array)\n",
        "                \n",
        "                if emb_array.ndim == 0:\n",
        "                    dim = 1\n",
        "                elif emb_array.ndim == 1:\n",
        "                    dim = emb_array.shape[0]\n",
        "                else:\n",
        "                    dim = emb_array.size\n",
        "                \n",
        "                if dim is not None and dim > 0:\n",
        "                    break\n",
        "    \n",
        "    if dim is None or dim == 0:\n",
        "        dim = 1  # 默认维度\n",
        "        print(f\"警告: 无法确定 {key} 的维度，使用默认值 1\")\n",
        "    else:\n",
        "        print(f\"  {key}: {dim} 维\")\n",
        "    \n",
        "    embedding_dims[key] = dim\n",
        "    input_dims.append(dim)\n",
        "\n",
        "print(f\"\\nEmbedding维度: {dict(zip(modality_keys, input_dims))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 划分数据集\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练集: 117 个患者\n",
            "验证集: 39 个患者\n",
            "测试集: 39 个患者\n"
          ]
        }
      ],
      "source": [
        "# 划分数据集\n",
        "patient_ids = list(patient_data.keys())\n",
        "train_ids, temp_ids = train_test_split(patient_ids, test_size=TEST_SIZE + VAL_SIZE, random_state=42)\n",
        "val_ids, test_ids = train_test_split(temp_ids, test_size=TEST_SIZE / (TEST_SIZE + VAL_SIZE), random_state=42)\n",
        "\n",
        "train_data = {pid: patient_data[pid] for pid in train_ids}\n",
        "val_data = {pid: patient_data[pid] for pid in val_ids}\n",
        "test_data = {pid: patient_data[pid] for pid in test_ids}\n",
        "\n",
        "train_labels = {pid: patient_labels[pid] for pid in train_ids}\n",
        "val_labels = {pid: patient_labels[pid] for pid in val_ids}\n",
        "test_labels = {pid: patient_labels[pid] for pid in test_ids}\n",
        "\n",
        "print(f\"训练集: {len(train_ids)} 个患者\")\n",
        "print(f\"验证集: {len(val_ids)} 个患者\")\n",
        "print(f\"测试集: {len(test_ids)} 个患者\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据集创建完成！\n"
          ]
        }
      ],
      "source": [
        "# 创建数据集\n",
        "train_dataset = MultiModalDataset(train_data, train_labels, modality_keys, embedding_dims)\n",
        "val_dataset = MultiModalDataset(val_data, val_labels, modality_keys, embedding_dims)\n",
        "test_dataset = MultiModalDataset(test_data, test_labels, modality_keys, embedding_dims)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(\"数据集创建完成！\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. 创建和训练模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型参数数量: 3090433\n",
            "模型已移动到设备: cuda\n"
          ]
        }
      ],
      "source": [
        "# 创建模型\n",
        "device = torch.device(DEVICE)\n",
        "model = MedFuseLSTM(\n",
        "    input_dims=input_dims,\n",
        "    hidden_dim=HIDDEN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dropout=DROPOUT,\n",
        ").to(device)\n",
        "\n",
        "print(f\"模型参数数量: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"模型已移动到设备: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "训练函数定义完成！\n"
          ]
        }
      ],
      "source": [
        "# 定义训练函数\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    num_epochs: int = 50,\n",
        "    lr: float = 0.001,\n",
        ") -> Tuple[nn.Module, List[float], List[float]]:\n",
        "    \"\"\"训练模型\"\"\"\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)\n",
        "    \n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        # 训练\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
        "            embeddings = [e.to(device) for e in batch['embeddings']]\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(embeddings)\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        \n",
        "        # 验证\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
        "                embeddings = [e.to(device) for e in batch['embeddings']]\n",
        "                labels = batch['label'].to(device)\n",
        "                \n",
        "                outputs = model(embeddings)\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        \n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
        "    \n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "    \n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "print(\"训练函数定义完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 60.83it/s]\n",
            "Epoch 1/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 234.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss=0.6997, Val Loss=0.6922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 100.16it/s]\n",
            "Epoch 2/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 170.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss=0.6947, Val Loss=0.6962\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 83.87it/s]\n",
            "Epoch 3/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 198.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss=0.6914, Val Loss=0.6989\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 72.21it/s]\n",
            "Epoch 4/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 190.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train Loss=0.6872, Val Loss=0.7105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 77.12it/s]\n",
            "Epoch 5/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 186.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Train Loss=0.6867, Val Loss=0.7150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 72.59it/s]\n",
            "Epoch 6/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train Loss=0.6862, Val Loss=0.7189\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 76.76it/s]\n",
            "Epoch 7/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Train Loss=0.6848, Val Loss=0.7195\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 72.72it/s]\n",
            "Epoch 8/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 162.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Train Loss=0.6840, Val Loss=0.7177\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 78.45it/s]\n",
            "Epoch 9/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 166.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Train Loss=0.6789, Val Loss=0.7173\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 78.67it/s]\n",
            "Epoch 10/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss=0.6850, Val Loss=0.7154\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.39it/s]\n",
            "Epoch 11/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Train Loss=0.6717, Val Loss=0.7105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.91it/s]\n",
            "Epoch 12/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 168.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Train Loss=0.6805, Val Loss=0.7163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.59it/s]\n",
            "Epoch 13/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Train Loss=0.6773, Val Loss=0.7212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 74.21it/s]\n",
            "Epoch 14/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 177.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Train Loss=0.6763, Val Loss=0.7169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.47it/s]\n",
            "Epoch 15/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Train Loss=0.6679, Val Loss=0.7074\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 79.03it/s]\n",
            "Epoch 16/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 190.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Train Loss=0.6708, Val Loss=0.7057\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 77.12it/s]\n",
            "Epoch 17/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 164.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Train Loss=0.6702, Val Loss=0.7032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 76.94it/s]\n",
            "Epoch 18/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 181.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train Loss=0.6700, Val Loss=0.6988\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 77.13it/s]\n",
            "Epoch 19/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 181.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Train Loss=0.6724, Val Loss=0.6911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 80.26it/s]\n",
            "Epoch 20/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 190.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Train Loss=0.6728, Val Loss=0.6881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 80.00it/s]\n",
            "Epoch 21/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 190.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21: Train Loss=0.6598, Val Loss=0.6805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.63it/s]\n",
            "Epoch 22/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 179.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22: Train Loss=0.6599, Val Loss=0.6751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 80.77it/s]\n",
            "Epoch 23/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 172.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23: Train Loss=0.6545, Val Loss=0.6649\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.33it/s]\n",
            "Epoch 24/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 196.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24: Train Loss=0.6476, Val Loss=0.6513\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 72.67it/s]\n",
            "Epoch 25/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 186.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25: Train Loss=0.6517, Val Loss=0.6466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 79.85it/s]\n",
            "Epoch 26/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 171.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26: Train Loss=0.6431, Val Loss=0.6523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 81.12it/s]\n",
            "Epoch 27/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27: Train Loss=0.6373, Val Loss=0.6385\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 83.83it/s]\n",
            "Epoch 28/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 154.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28: Train Loss=0.6316, Val Loss=0.6212\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 74.38it/s]\n",
            "Epoch 29/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 177.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29: Train Loss=0.6264, Val Loss=0.6153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 82.58it/s]\n",
            "Epoch 30/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 143.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30: Train Loss=0.6193, Val Loss=0.6104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 75.03it/s]\n",
            "Epoch 31/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 158.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31: Train Loss=0.6092, Val Loss=0.5928\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 32/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 56.19it/s]\n",
            "Epoch 32/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 105.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32: Train Loss=0.6179, Val Loss=0.5778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 33/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 58.15it/s]\n",
            "Epoch 33/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 141.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33: Train Loss=0.6005, Val Loss=0.5617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 34/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 72.69it/s]\n",
            "Epoch 34/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34: Train Loss=0.5992, Val Loss=0.5634\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 35/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 79.92it/s]\n",
            "Epoch 35/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 189.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35: Train Loss=0.5969, Val Loss=0.5795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 36/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 80.56it/s]\n",
            "Epoch 36/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36: Train Loss=0.5957, Val Loss=0.5500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 37/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 73.31it/s]\n",
            "Epoch 37/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37: Train Loss=0.6024, Val Loss=0.5317\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 38/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 71.09it/s]\n",
            "Epoch 38/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 159.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38: Train Loss=0.5706, Val Loss=0.5453\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 39/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 72.41it/s]\n",
            "Epoch 39/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 179.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39: Train Loss=0.5850, Val Loss=0.5287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 40/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 63.31it/s]\n",
            "Epoch 40/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40: Train Loss=0.5888, Val Loss=0.5308\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 41/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 67.83it/s]\n",
            "Epoch 41/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 159.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41: Train Loss=0.5668, Val Loss=0.5309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 42/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 69.42it/s]\n",
            "Epoch 42/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 159.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42: Train Loss=0.5494, Val Loss=0.5306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 43/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 69.16it/s]\n",
            "Epoch 43/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43: Train Loss=0.5639, Val Loss=0.5309\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 44/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 66.92it/s]\n",
            "Epoch 44/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 153.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44: Train Loss=0.5526, Val Loss=0.5412\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 45/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 70.66it/s]\n",
            "Epoch 45/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 155.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45: Train Loss=0.5753, Val Loss=0.5481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 46/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 71.96it/s]\n",
            "Epoch 46/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 165.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46: Train Loss=0.5396, Val Loss=0.5354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 47/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 75.03it/s]\n",
            "Epoch 47/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 186.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47: Train Loss=0.5543, Val Loss=0.5314\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 48/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 77.16it/s]\n",
            "Epoch 48/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 173.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48: Train Loss=0.5378, Val Loss=0.5331\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 49/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 79.33it/s]\n",
            "Epoch 49/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 189.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49: Train Loss=0.5393, Val Loss=0.5316\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 50/50 [Train]: 100%|██████████| 4/4 [00:00<00:00, 69.52it/s]\n",
            "Epoch 50/50 [Val]: 100%|██████████| 2/2 [00:00<00:00, 189.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50: Train Loss=0.5320, Val Loss=0.5421\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 开始训练\n",
        "model, train_losses, val_losses = train_model(\n",
        "    model, train_loader, val_loader, device, NUM_EPOCHS, LR\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. 评估模型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "评估函数定义完成！\n"
          ]
        }
      ],
      "source": [
        "# 定义评估函数\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    test_loader: DataLoader,\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"评估模型，返回ROC AUC、预测概率和真实标签\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            embeddings = [e.to(device) for e in batch['embeddings']]\n",
        "            labels = batch['label'].cpu().numpy()\n",
        "            \n",
        "            outputs = model(embeddings)\n",
        "            probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n",
        "            \n",
        "            all_preds.extend(probs)\n",
        "            all_labels.extend(labels)\n",
        "    \n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "    \n",
        "    if len(np.unique(all_labels)) < 2:\n",
        "        print(\"警告: 测试集中只有一个类别，无法计算ROC AUC\")\n",
        "        return 0.0, all_preds, all_labels\n",
        "    \n",
        "    auc = roc_auc_score(all_labels, all_preds)\n",
        "    return auc, all_preds, all_labels\n",
        "\n",
        "print(\"评估函数定义完成！\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 104.29it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 128.89it/s]\n",
            "Evaluating: 100%|██████████| 2/2 [00:00<00:00, 128.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "评估结果 (CT - 进展)\n",
            "============================================================\n",
            "训练集 ROC AUC: 0.8033\n",
            "验证集 ROC AUC: 0.7315\n",
            "测试集 ROC AUC: 0.6176\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 评估模型（包括训练集、验证集和测试集）\n",
        "def evaluate_all_sets(model, train_loader, val_loader, test_loader, device):\n",
        "    \"\"\"评估所有数据集\"\"\"\n",
        "    train_auc, train_preds, train_labels = evaluate_model(model, train_loader, device)\n",
        "    val_auc, val_preds, val_labels = evaluate_model(model, val_loader, device)\n",
        "    test_auc, test_preds, test_labels = evaluate_model(model, test_loader, device)\n",
        "    \n",
        "    return {\n",
        "        'train': (train_auc, train_preds, train_labels),\n",
        "        'val': (val_auc, val_preds, val_labels),\n",
        "        'test': (test_auc, test_preds, test_labels),\n",
        "    }\n",
        "\n",
        "# 评估所有数据集\n",
        "results_all = evaluate_all_sets(model, train_loader, val_loader, test_loader, device)\n",
        "\n",
        "train_auc, train_preds, train_labels = results_all['train']\n",
        "val_auc, val_preds, val_labels = results_all['val']\n",
        "test_auc, test_preds, test_labels = results_all['test']\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"评估结果 ({MODALITY.upper()} - {label_name})\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"训练集 ROC AUC: {train_auc:.4f}\")\n",
        "print(f\"验证集 ROC AUC: {val_auc:.4f}\")\n",
        "print(f\"测试集 ROC AUC: {test_auc:.4f}\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 保存结果\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "结果已保存到 fusion_results\\CT\\progress\n",
            "  - results_progress.json\n",
            "  - train_predictions_progress.csv\n",
            "  - val_predictions_progress.csv\n",
            "  - test_predictions_progress.csv\n"
          ]
        }
      ],
      "source": [
        "# 创建输出目录\n",
        "output_dir = Path(OUTPUT_DIR) / MODALITY.upper() / LABEL_TYPE\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 保存结果\n",
        "results = {\n",
        "    \"modality\": MODALITY.upper(),\n",
        "    \"label_type\": LABEL_TYPE,\n",
        "    \"train_auc\": float(train_auc),\n",
        "    \"val_auc\": float(val_auc),\n",
        "    \"test_auc\": float(test_auc),\n",
        "    \"num_train\": len(train_ids),\n",
        "    \"num_val\": len(val_ids),\n",
        "    \"num_test\": len(test_ids),\n",
        "}\n",
        "\n",
        "with open(output_dir / f\"results_{LABEL_TYPE}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# 保存所有数据集的预测结果\n",
        "# 训练集\n",
        "train_results_df = pd.DataFrame({\n",
        "    \"patient_id\": train_ids,\n",
        "    \"true_label\": train_labels,\n",
        "    \"pred_prob\": train_preds,\n",
        "})\n",
        "train_results_df.to_csv(output_dir / f\"train_predictions_{LABEL_TYPE}.csv\", index=False)\n",
        "\n",
        "# 验证集\n",
        "val_results_df = pd.DataFrame({\n",
        "    \"patient_id\": val_ids,\n",
        "    \"true_label\": val_labels,\n",
        "    \"pred_prob\": val_preds,\n",
        "})\n",
        "val_results_df.to_csv(output_dir / f\"val_predictions_{LABEL_TYPE}.csv\", index=False)\n",
        "\n",
        "# 测试集\n",
        "test_results_df = pd.DataFrame({\n",
        "    \"patient_id\": test_ids,\n",
        "    \"true_label\": test_labels,\n",
        "    \"pred_prob\": test_preds,\n",
        "})\n",
        "test_results_df.to_csv(output_dir / f\"test_predictions_{LABEL_TYPE}.csv\", index=False)\n",
        "\n",
        "print(f\"\\n结果已保存到 {output_dir}\")\n",
        "print(f\"  - results_{LABEL_TYPE}.json\")\n",
        "print(f\"  - train_predictions_{LABEL_TYPE}.csv\")\n",
        "print(f\"  - val_predictions_{LABEL_TYPE}.csv\")\n",
        "print(f\"  - test_predictions_{LABEL_TYPE}.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 运行所有任务（CT/PET × 进展/死亡）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 16:13:30,045 - INFO - 加载 sam_med2d from .\\embeddings_output_CT.xlsx\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "开始运行所有任务\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "任务 1/4: CT - 进展\n",
            "============================================================\n",
            "定义的embedding文件路径 (CT):\n",
            "  sam_med2d: .\\embeddings_output_CT.xlsx\n",
            "  medclip: .\\medclip_embeddings\\medclip_embeddings_ct_vit.csv\n",
            "  radfm: .\\ct_embeddings_tokens.csv\n",
            "找到 3 个embedding文件\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-29 16:14:12,734 - INFO -   Excel文件列名: ['医院', '患者名称', 'file_name', 'embedding_dim', 'embedding_0', 'embedding_1', 'embedding_2', 'embedding_3', 'embedding_4', 'embedding_5']... (共16383列)\n",
            "2025-11-29 16:14:12,735 - INFO -   使用患者名称列: 患者名称\n",
            "2025-11-29 16:14:16,887 - INFO -   sam_med2d: 加载了 163 个唯一患者\n",
            "2025-11-29 16:14:16,888 - INFO -   sam_med2d: 加载了 163 个唯一患者\n",
            "2025-11-29 16:14:16,889 - INFO - 加载 medclip from .\\medclip_embeddings\\medclip_embeddings_ct_vit.csv\n",
            "2025-11-29 16:14:16,900 - INFO -   CSV文件列名: ['patient_name', 'center', 'embedding_000', 'embedding_001', 'embedding_002', 'embedding_003', 'embedding_004', 'embedding_005', 'embedding_006', 'embedding_007']... (共514列)\n",
            "2025-11-29 16:14:16,902 - INFO -   使用患者名称列: patient_name\n",
            "2025-11-29 16:14:19,741 - INFO -   medclip: 加载了 163 个唯一患者\n",
            "2025-11-29 16:14:19,742 - INFO - 加载 radfm from .\\ct_embeddings_tokens.csv\n",
            "2025-11-29 16:14:19,800 - INFO -   CSV文件列名: ['中心', '患者名', 'file_key', 'feature_0', 'feature_1', 'feature_2', 'feature_3', 'feature_4', 'feature_5', 'feature_6']... (共5123列)\n",
            "2025-11-29 16:14:19,802 - INFO -   使用患者名称列: 患者名\n",
            "2025-11-29 16:14:23,839 - INFO -   radfm: 加载了 163 个唯一患者\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "加载了 2 个患者的embedding数据\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'items'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[109], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m patient_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     71\u001b[0m patient_labels \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m patient_id, emb_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[43mall_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m patient_id \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m     75\u001b[0m         patient_data[patient_id] \u001b[38;5;241m=\u001b[39m emb_dict\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
          ]
        }
      ],
      "source": [
        "# ========== 运行所有任务 ==========\n",
        "# 定义所有任务\n",
        "ALL_TASKS = [\n",
        "    {\"modality\": \"ct\", \"label_type\": \"progress\", \"label_name\": \"进展\"},\n",
        "    {\"modality\": \"ct\", \"label_type\": \"death\", \"label_name\": \"死亡\"},\n",
        "    {\"modality\": \"pet\", \"label_type\": \"progress\", \"label_name\": \"进展\"},\n",
        "    {\"modality\": \"pet\", \"label_type\": \"death\", \"label_name\": \"死亡\"},\n",
        "]\n",
        "\n",
        "# 存储所有任务的结果\n",
        "all_results_summary = []\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"开始运行所有任务\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for task_idx, task in enumerate(ALL_TASKS, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"任务 {task_idx}/4: {task['modality'].upper()} - {task['label_name']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # 设置当前任务参数\n",
        "    MODALITY = task[\"modality\"]\n",
        "    LABEL_TYPE = task[\"label_type\"]\n",
        "    label_name = task[\"label_name\"]\n",
        "    \n",
        "    # 定义embedding文件路径（SAM-Med2D使用xlsx，其他使用CSV）\n",
        "    if MODALITY.lower() == \"ct\":\n",
        "        embedding_paths = {\n",
        "            \"sam_med2d\": os.path.join(EMBEDDING_DIR, \"embeddings_output_CT.xlsx\"),  # 注意是复数形式embeddings\n",
        "            \"medclip\": os.path.join(EMBEDDING_DIR, \"medclip_embeddings\", \"medclip_embeddings_ct_vit.csv\"),\n",
        "            \"radfm\": os.path.join(EMBEDDING_DIR, \"ct_embeddings_tokens.csv\"),\n",
        "        }\n",
        "    else:  # PET\n",
        "        embedding_paths = {\n",
        "            \"sam_med2d\": os.path.join(EMBEDDING_DIR, \"embeddings_output.xlsx\"),  # 注意是复数形式embeddings\n",
        "            \"medclip\": os.path.join(EMBEDDING_DIR, \"medclip_embeddings\", \"medclip_embeddings_vit_dropcol.csv\"),\n",
        "            \"radfm\": os.path.join(EMBEDDING_DIR, \"pet_embedding_tokens_with_labels.csv\"),\n",
        "        }\n",
        "    \n",
        "    print(f\"定义的embedding文件路径 ({MODALITY.upper()}):\")\n",
        "    for emb_name, path in embedding_paths.items():\n",
        "        print(f\"  {emb_name}: {path}\")\n",
        "    \n",
        "    # 检查文件是否存在\n",
        "    existing_paths = {}\n",
        "    for emb_name, path in embedding_paths.items():\n",
        "        if os.path.exists(path):\n",
        "            existing_paths[emb_name] = path\n",
        "        else:\n",
        "            print(f\"警告: 未找到 {emb_name} 文件: {path}\")\n",
        "    \n",
        "    if not existing_paths:\n",
        "        print(f\"警告: 未找到任何embedding文件，跳过此任务\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"找到 {len(existing_paths)} 个embedding文件\")\n",
        "    \n",
        "    # 加载embeddings（注意返回的是元组）\n",
        "    all_embeddings, emb_name_mapping = load_embeddings(existing_paths)\n",
        "    print(f\"加载了 {len(all_embeddings)} 个患者的embedding数据\")\n",
        "    \n",
        "    # 选择标签\n",
        "    if LABEL_TYPE == \"progress\":\n",
        "        labels = progress_labels\n",
        "    else:\n",
        "        labels = death_labels\n",
        "    \n",
        "    # 准备数据\n",
        "    patient_data = {}\n",
        "    patient_labels = {}\n",
        "    \n",
        "    for patient_id, emb_dict in all_embeddings.items():\n",
        "        if patient_id in labels:\n",
        "            patient_data[patient_id] = emb_dict\n",
        "            patient_labels[patient_id] = labels[patient_id]\n",
        "    \n",
        "    if not patient_data:\n",
        "        print(f\"警告: 没有找到匹配的患者数据，跳过此任务\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"准备训练数据: {len(patient_data)} 个患者 ({label_name})\")\n",
        "    \n",
        "    # 获取embedding维度\n",
        "    modality_keys = list(existing_paths.keys())\n",
        "    embedding_dims = {}\n",
        "    input_dims = []\n",
        "    \n",
        "    for key in modality_keys:\n",
        "        dim = None\n",
        "        for patient_id, emb_dict in patient_data.items():\n",
        "            if key in emb_dict and emb_dict[key] is not None:\n",
        "                emb_array = emb_dict[key]\n",
        "                if not isinstance(emb_array, np.ndarray):\n",
        "                    emb_array = np.array(emb_array)\n",
        "                if emb_array.ndim > 1:\n",
        "                    dim = emb_array.size\n",
        "                else:\n",
        "                    dim = emb_array.shape[0]\n",
        "                break\n",
        "        \n",
        "        if dim is None:\n",
        "            dim = 1\n",
        "        embedding_dims[key] = dim\n",
        "        input_dims.append(dim)\n",
        "    \n",
        "    print(f\"Embedding维度: {dict(zip(modality_keys, input_dims))}\")\n",
        "    \n",
        "    # 划分数据集\n",
        "    patient_ids = list(patient_data.keys())\n",
        "    train_ids, temp_ids = train_test_split(patient_ids, test_size=TEST_SIZE + VAL_SIZE, random_state=42)\n",
        "    val_ids, test_ids = train_test_split(temp_ids, test_size=TEST_SIZE / (TEST_SIZE + VAL_SIZE), random_state=42)\n",
        "    \n",
        "    train_data = {pid: patient_data[pid] for pid in train_ids}\n",
        "    val_data = {pid: patient_data[pid] for pid in val_ids}\n",
        "    test_data = {pid: patient_data[pid] for pid in test_ids}\n",
        "    \n",
        "    train_labels_dict = {pid: patient_labels[pid] for pid in train_ids}\n",
        "    val_labels_dict = {pid: patient_labels[pid] for pid in val_ids}\n",
        "    test_labels_dict = {pid: patient_labels[pid] for pid in test_ids}\n",
        "    \n",
        "    # 创建数据集\n",
        "    train_dataset = MultiModalDataset(train_data, train_labels_dict, modality_keys, embedding_dims)\n",
        "    val_dataset = MultiModalDataset(val_data, val_labels_dict, modality_keys, embedding_dims)\n",
        "    test_dataset = MultiModalDataset(test_data, test_labels_dict, modality_keys, embedding_dims)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "    \n",
        "    # 创建模型\n",
        "    device = torch.device(DEVICE)\n",
        "    model = MedFuseLSTM(\n",
        "        input_dims=input_dims,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        num_layers=NUM_LAYERS,\n",
        "        dropout=DROPOUT,\n",
        "    ).to(device)\n",
        "    \n",
        "    print(f\"模型参数数量: {sum(p.numel() for p in model.parameters())}\")\n",
        "    \n",
        "    # 训练\n",
        "    print(\"开始训练...\")\n",
        "    model, train_losses, val_losses = train_model(\n",
        "        model, train_loader, val_loader, device, NUM_EPOCHS, LR\n",
        "    )\n",
        "    \n",
        "    # 评估\n",
        "    print(\"开始评估...\")\n",
        "    results_all = evaluate_all_sets(model, train_loader, val_loader, test_loader, device)\n",
        "    \n",
        "    train_auc, train_preds, train_labels_arr = results_all['train']\n",
        "    val_auc, val_preds, val_labels_arr = results_all['val']\n",
        "    test_auc, test_preds, test_labels_arr = results_all['test']\n",
        "    \n",
        "    print(f\"\\n评估结果:\")\n",
        "    print(f\"  训练集 ROC AUC: {train_auc:.4f}\")\n",
        "    print(f\"  验证集 ROC AUC: {val_auc:.4f}\")\n",
        "    print(f\"  测试集 ROC AUC: {test_auc:.4f}\")\n",
        "    \n",
        "    # 保存结果\n",
        "    output_dir = Path(OUTPUT_DIR) / MODALITY.upper() / LABEL_TYPE\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    results = {\n",
        "        \"modality\": MODALITY.upper(),\n",
        "        \"label_type\": LABEL_TYPE,\n",
        "        \"train_auc\": float(train_auc),\n",
        "        \"val_auc\": float(val_auc),\n",
        "        \"test_auc\": float(test_auc),\n",
        "        \"num_train\": len(train_ids),\n",
        "        \"num_val\": len(val_ids),\n",
        "        \"num_test\": len(test_ids),\n",
        "    }\n",
        "    \n",
        "    with open(output_dir / f\"results_{LABEL_TYPE}.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "    \n",
        "    # 保存所有数据集的预测结果\n",
        "    train_results_df = pd.DataFrame({\n",
        "        \"patient_id\": train_ids,\n",
        "        \"true_label\": train_labels_arr,\n",
        "        \"pred_prob\": train_preds,\n",
        "    })\n",
        "    train_results_df.to_csv(output_dir / f\"train_predictions_{LABEL_TYPE}.csv\", index=False)\n",
        "    \n",
        "    val_results_df = pd.DataFrame({\n",
        "        \"patient_id\": val_ids,\n",
        "        \"true_label\": val_labels_arr,\n",
        "        \"pred_prob\": val_preds,\n",
        "    })\n",
        "    val_results_df.to_csv(output_dir / f\"val_predictions_{LABEL_TYPE}.csv\", index=False)\n",
        "    \n",
        "    test_results_df = pd.DataFrame({\n",
        "        \"patient_id\": test_ids,\n",
        "        \"true_label\": test_labels_arr,\n",
        "        \"pred_prob\": test_preds,\n",
        "    })\n",
        "    test_results_df.to_csv(output_dir / f\"test_predictions_{LABEL_TYPE}.csv\", index=False)\n",
        "    \n",
        "    print(f\"结果已保存到 {output_dir}\")\n",
        "    \n",
        "    # 记录到总结\n",
        "    all_results_summary.append({\n",
        "        \"modality\": MODALITY.upper(),\n",
        "        \"label_type\": LABEL_TYPE,\n",
        "        \"label_name\": label_name,\n",
        "        \"train_auc\": float(train_auc),\n",
        "        \"val_auc\": float(val_auc),\n",
        "        \"test_auc\": float(test_auc),\n",
        "    })\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"所有任务完成！\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# 打印总结\n",
        "print(\"\\n结果总结:\")\n",
        "print(\"-\" * 60)\n",
        "for result in all_results_summary:\n",
        "    print(f\"{result['modality']} - {result['label_name']}:\")\n",
        "    print(f\"  训练集 AUC: {result['train_auc']:.4f}\")\n",
        "    print(f\"  验证集 AUC: {result['val_auc']:.4f}\")\n",
        "    print(f\"  测试集 AUC: {result['test_auc']:.4f}\")\n",
        "    print()\n",
        "\n",
        "# 保存总结\n",
        "summary_df = pd.DataFrame(all_results_summary)\n",
        "summary_df.to_csv(Path(OUTPUT_DIR) / \"all_results_summary.csv\", index=False)\n",
        "print(f\"总结已保存到 {Path(OUTPUT_DIR) / 'all_results_summary.csv'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cdml",
      "language": "python",
      "name": "cdml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
