{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b6ee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\24088\\.conda\\envs\\cdml\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Get CT NIfTI image embeddings using MedCLIP.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Callable, Iterable, Iterator, List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "try:\n",
    "    from medclip import (\n",
    "        MedCLIPModel,\n",
    "        MedCLIPVisionModelViT,\n",
    "        constants as medclip_constants,\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"please install medclip: pip install medclip\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93344732",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "defalut pass\n",
    "\"\"\"\n",
    "DEFAULT_INPUT_DIRS = [\n",
    "    r\"D:\\research\\cd-ml\\(process)nzhf\\提取的单个nii文件\",\n",
    "]\n",
    "\n",
    "DEFAULT_OUTPUT_DIR = r\"D:\\research\\cd-ml\\(process)nzhf\\medclip_embeddings\"\n",
    "DEFAULT_BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f85c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data class\n",
    "\"\"\"\n",
    "@dataclass\n",
    "class ImageItem:\n",
    "    identifier: str\n",
    "    image: Image.Image\n",
    "    patient_name: str\n",
    "    center: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c5585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read and normalize nifti file\n",
    "\"\"\"\n",
    "def nifti_to_images(\n",
    "    nifti_path: Path,\n",
    "    center_name: str,\n",
    "    patient_name: str,\n",
    ") -> List[ImageItem]:\n",
    "    \"\"\"change nifti file to images\"\"\"\n",
    "    volume = nib.load(str(nifti_path))\n",
    "    data = volume.get_fdata().astype(np.float32)\n",
    "\n",
    "    if data.ndim == 2:\n",
    "        data = data[..., np.newaxis]\n",
    "    elif data.ndim == 4:\n",
    "        data = np.moveaxis(data, 0, -1)\n",
    "\n",
    "    images: List[ImageItem] = []\n",
    "    num_slices = data.shape[-1]\n",
    "    for idx in range(num_slices):\n",
    "        frame = data[..., idx]\n",
    "        frame = frame.astype(np.float32)\n",
    "        frame -= frame.min()\n",
    "        max_val = frame.max()\n",
    "        if max_val > 0:\n",
    "            frame /= max_val\n",
    "        frame_uint8 = (frame * 255).clip(0, 255).astype(np.uint8)\n",
    "        pil_img = Image.fromarray(frame_uint8).convert(\"RGB\")\n",
    "        identifier = f\"{nifti_path}::slice_{idx:04d}\"\n",
    "        images.append(\n",
    "            ImageItem(\n",
    "                identifier=identifier,\n",
    "                image=pil_img,\n",
    "                patient_name=patient_name,\n",
    "                center=center_name,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a8420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"file iter\"\"\"\n",
    "NIFTI_EXTENSIONS = (\".nii\", \".nii.gz\")\n",
    "\n",
    "def iter_nifti_files(input_dirs: Sequence[Path]) -> Iterator[Tuple[Path, str, str]]:\n",
    "\n",
    "    for directory in input_dirs:\n",
    "        dir_path = Path(directory)\n",
    "        if not dir_path.exists():\n",
    "            continue\n",
    "        \n",
    "        for center_dir in dir_path.iterdir():\n",
    "            if not center_dir.is_dir():\n",
    "                continue\n",
    "            center_name = center_dir.name\n",
    "            \n",
    "            for patient_dir in center_dir.iterdir():\n",
    "                if not patient_dir.is_dir():\n",
    "                    continue\n",
    "                patient_name = patient_dir.name\n",
    "                \n",
    "                for file_path in patient_dir.iterdir():\n",
    "                    if not file_path.is_file():\n",
    "                        continue\n",
    "                    lower = file_path.name.lower()\n",
    "                    if lower.endswith(NIFTI_EXTENSIONS) and \"ct\" in lower:\n",
    "                        yield file_path, center_name, patient_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a4ada2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"medclip model\"\"\"\n",
    "def _resolve_pretrained_dir(model: MedCLIPModel, input_dir: Optional[str] = None) -> Tuple[Path, str]:\n",
    "    \"\"\"according to model type, return pretrained weight dir and download url\"\"\"\n",
    "    if isinstance(model.vision_model, MedCLIPVisionModelViT):\n",
    "        pretrained_url = medclip_constants.PRETRAINED_URL_MEDCLIP_VIT\n",
    "        default_dir = Path(\"./pretrained/medclip-vit\")\n",
    "    else:\n",
    "        pretrained_url = medclip_constants.PRETRAINED_URL_MEDCLIP_RESNET\n",
    "        default_dir = Path(\"./pretrained/medclip-resnet\")\n",
    "\n",
    "    if input_dir is not None:\n",
    "        default_dir = Path(input_dir)\n",
    "    return default_dir, pretrained_url\n",
    "\n",
    "def load_medclip_pretrained(model: MedCLIPModel, input_dir: Optional[str] = None) -> None:\n",
    "    \"\"\"custom load pretrained weight\"\"\"\n",
    "    from zipfile import ZipFile\n",
    "    import requests\n",
    "    import wget\n",
    "\n",
    "    weight_dir, pretrained_url = _resolve_pretrained_dir(model, input_dir)\n",
    "    weight_dir = weight_dir.resolve()\n",
    "    weight_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    weight_file = weight_dir / medclip_constants.WEIGHTS_NAME\n",
    "    if not weight_file.exists():\n",
    "        response = requests.get(pretrained_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        download_url = response.text.strip()\n",
    "        zip_path = wget.download(download_url, str(weight_dir))\n",
    "        with ZipFile(zip_path) as zipf:\n",
    "            zipf.extractall(weight_dir)\n",
    "        print(f\"\\n download pretrained model: {download_url}\")\n",
    "\n",
    "    state_dict = torch.load(weight_file, map_location=\"cpu\")\n",
    "    state_dict.pop(\"text_model.model.embeddings.position_ids\", None)\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if missing or unexpected:\n",
    "        logging.warning(\"ignore missing=%s unexpected=%s\", missing, unexpected)\n",
    "    print(f\"load model weight from: {weight_dir}\")\n",
    "\n",
    "def build_model(device: torch.device) -> Tuple[MedCLIPModel, Callable[[Image.Image], torch.Tensor]]:\n",
    "    vision_cls = MedCLIPVisionModelViT\n",
    "    model = MedCLIPModel(vision_cls=vision_cls)\n",
    "    load_medclip_pretrained(model)\n",
    "    model = model.to(device).eval()\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[medclip_constants.IMG_MEAN],\n",
    "        std=[medclip_constants.IMG_STD],\n",
    "    )\n",
    "    target_size = medclip_constants.IMG_SIZE\n",
    "\n",
    "    def preprocess(image: Image.Image) -> torch.Tensor:\n",
    "        if image.mode != \"L\":\n",
    "            image = image.convert(\"L\")\n",
    "        width, height = image.size\n",
    "        canvas_size = max(target_size, width, height)\n",
    "        canvas = Image.new(\"L\", (canvas_size, canvas_size), color=0)\n",
    "        offset = ((canvas_size - width) // 2, (canvas_size - height) // 2)\n",
    "        canvas.paste(image, offset)\n",
    "        canvas = canvas.resize((target_size, target_size), Image.BICUBIC)\n",
    "        tensor = TF.to_tensor(canvas)\n",
    "        tensor = normalize(tensor)\n",
    "        return tensor\n",
    "\n",
    "    return model, preprocess\n",
    "\n",
    "def embed_images(\n",
    "    model: MedCLIPModel,\n",
    "    preprocess: Callable[[Image.Image], torch.Tensor],\n",
    "    device: torch.device,\n",
    "    items: Sequence[ImageItem],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"batch get embeddings (unit vector)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pixel_values = torch.stack([preprocess(item.image) for item in items]).to(device)\n",
    "        embeddings = model.encode_image(pixel_values=pixel_values)\n",
    "    return embeddings.cpu().numpy().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3676a95",
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"main process\"\"\"\n",
    "def parse_args(argv: Optional[Sequence[str]] = None) -> argparse.Namespace:\n",
    "    parser = argparse.ArgumentParser(description=\"use medclip to extract ct nifti image embedding\")\n",
    "    parser.add_argument(\n",
    "        \"--input-dirs\",\n",
    "        nargs=\"+\",\n",
    "        default=DEFAULT_INPUT_DIRS,\n",
    "        help=\"need to traverse folder (can be multiple)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        default=DEFAULT_OUTPUT_DIR,\n",
    "        help=\"output folder, save embeddings and index\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=DEFAULT_BATCH_SIZE,\n",
    "        help=\"the number of images to send to medclip\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device\",\n",
    "        default=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        help=\"use device: cuda / cpu\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--progress\",\n",
    "        action=\"store_true\",\n",
    "        help=\"show tqdm progress bar\",\n",
    "    )\n",
    "    if argv is None and \"ipykernel\" in sys.modules:\n",
    "        argv = []\n",
    "    return parser.parse_args(argv)\n",
    "\n",
    "def main() -> None:\n",
    "    args = parse_args()\n",
    "    input_dirs = [Path(p).resolve() for p in args.input_dirs]\n",
    "    output_dir = Path(args.output_dir).resolve()\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    logging.info(\"input dirs: %s\", input_dirs)\n",
    "    logging.info(\"output dir: %s\", output_dir)\n",
    "\n",
    "    device = torch.device(args.device)\n",
    "    model, preprocess = build_model(device)\n",
    "    logging.info(\"loaded medclip model, use device: %s\", device)\n",
    "\n",
    "    batch_size = max(1, args.batch_size)\n",
    "    nifti_entries = list(iter_nifti_files(input_dirs))\n",
    "    if not nifti_entries:\n",
    "        logging.warning(\"no ct nifti file found in specified dirs\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"found %d ct nifti files\", len(nifti_entries))\n",
    "\n",
    "    embedding_blocks: List[np.ndarray] = []\n",
    "    identifiers: List[str] = []\n",
    "    patient_names: List[str] = []\n",
    "    centers: List[str] = []\n",
    "    errors: List[Tuple[str, str]] = []\n",
    "\n",
    "    iterator: Iterable[Tuple[Path, str, str]]\n",
    "    if args.progress:\n",
    "        iterator = tqdm(nifti_entries, desc=\"read NIfTI\", unit=\"file\")\n",
    "    else:\n",
    "        iterator = nifti_entries\n",
    "\n",
    "    buffer: List[ImageItem] = []\n",
    "    for nifti_path, center_name, patient_name in iterator:\n",
    "        try:\n",
    "            images = nifti_to_images(nifti_path, center_name, patient_name)\n",
    "            buffer.extend(images)\n",
    "        except Exception as exc:\n",
    "            errors.append((str(nifti_path), repr(exc)))\n",
    "            continue\n",
    "\n",
    "        while len(buffer) >= batch_size:\n",
    "            batch = buffer[:batch_size]\n",
    "            buffer = buffer[batch_size:]\n",
    "            embeddings = embed_images(model, preprocess, device, batch)\n",
    "            embedding_blocks.append(embeddings)\n",
    "            identifiers.extend(item.identifier for item in batch)\n",
    "            patient_names.extend(item.patient_name for item in batch)\n",
    "            centers.extend(item.center for item in batch)\n",
    "\n",
    "    if buffer:\n",
    "        embeddings = embed_images(model, preprocess, device, buffer)\n",
    "        embedding_blocks.append(embeddings)\n",
    "        identifiers.extend(item.identifier for item in buffer)\n",
    "        patient_names.extend(item.patient_name for item in buffer)\n",
    "        centers.extend(item.center for item in buffer)\n",
    "\n",
    "    all_embeddings = np.concatenate(embedding_blocks, axis=0) if embedding_blocks else np.zeros((0, 512), dtype=np.float32)\n",
    "\n",
    "    npz_path = output_dir / \"medclip_embeddings_ct_vit.npz\"\n",
    "    np.savez_compressed(\n",
    "        npz_path,\n",
    "        embeddings=all_embeddings,\n",
    "        identifiers=np.array(identifiers, dtype=object),\n",
    "        patient_names=np.array(patient_names, dtype=object),\n",
    "        centers=np.array(centers, dtype=object),\n",
    "    )\n",
    "    logging.info(\"save embeddings to %s, shape: %s\", npz_path, all_embeddings.shape)\n",
    "\n",
    "    embedding_cols = [f\"embedding_{i:03d}\" for i in range(all_embeddings.shape[1])]\n",
    "    dataframe = pd.DataFrame(all_embeddings, columns=embedding_cols, dtype=np.float32)\n",
    "    dataframe.insert(0, \"center\", centers)\n",
    "    dataframe.insert(0, \"patient_name\", patient_names)\n",
    "    csv_path = output_dir / \"medclip_embeddings_ct_vit.csv\"\n",
    "    dataframe.to_csv(csv_path, index=False)\n",
    "    logging.info(\"save csv to %s, shape: %s\", csv_path, dataframe.shape[1])\n",
    "\n",
    "    mapping_df = pd.DataFrame(\n",
    "        {\n",
    "            \"identifier\": identifiers,\n",
    "            \"patient_name\": patient_names,\n",
    "            \"center\": centers,\n",
    "        }\n",
    "    )\n",
    "    mapping_path = output_dir / \"medclip_embeddings_ct_mapping.csv\"\n",
    "    mapping_df.to_csv(mapping_path, index=False)\n",
    "    logging.info(\"save mapping to %s, shape: %s\", mapping_path, mapping_df.shape[0])\n",
    "\n",
    "    meta = {\n",
    "        \"input_dirs\": [str(p) for p in input_dirs],\n",
    "        \"model\": \"MedCLIP-ViT\",\n",
    "        \"batch_size\": batch_size,\n",
    "        \"device\": str(device),\n",
    "        \"num_files\": len(nifti_entries),\n",
    "        \"num_images\": len(identifiers),\n",
    "        \"npz_path\": str(npz_path),\n",
    "        \"csv_path\": str(csv_path),\n",
    "        \"mapping_path\": str(mapping_path),\n",
    "    }\n",
    "    meta_path = output_dir / \"medclip_embeddings_ct_meta.json\"\n",
    "    with meta_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "    logging.info(\"save meta to %s\", meta_path)\n",
    "\n",
    "    if errors:\n",
    "        error_log_path = output_dir / \"medclip_embedding_ct_errors.log\"\n",
    "        with error_log_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for path, err in errors:\n",
    "                f.write(f\"{path}\\t{err}\\n\")\n",
    "        logging.warning(\"save %d errors to %s\", len(errors), error_log_path)\n",
    "    else:\n",
    "        logging.info(\"save %d errors to %s\", len(errors), error_log_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdml",
   "language": "python",
   "name": "cdml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
